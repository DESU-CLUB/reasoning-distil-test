# reasoning-distil-test
Validate results from recent blogpost from Konstantin Kolomeitsev on distilling reasoning for &lt; $10, and also exploring more possibilities with the proposed architecture

# Credits

## Models Used
Credit for LLM Modules and the original source code to set up the training pipeline goes to Konstantin Kolomeitsev, who as posted a paper on this: https://arxiv.org/pdf/2502.08213 and has also written a blogpost about it here: https://kolomeitsev.space/how-to-teach-a-model-to-reason-without-overtraining-it-for-less-than-10-fb764846c11f

**More importantly**, the source code for the definitions and the entire training pipeline for the ideas presented in the paper has been kindly open-sourced by Konstantin here: https://huggingface.co/kkolomeitsev/llm-modules/tree/main 

## Dataset
The dataset used is Bespoke-Stratos-17k from Bespoke Labs

